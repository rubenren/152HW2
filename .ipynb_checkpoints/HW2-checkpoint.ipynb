{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 152: Intro to Computer Vision - Winter 2020 Assignment 2\n",
    "## Instructor: David Kriegman\n",
    "### Assignment published on Monday, February 3, 2020\n",
    "### Due on Friday, February 14, 2019 at 11:59pm\n",
    "\n",
    "## Instructions\n",
    "* This assignment must be completed individually. Review the academic integrity and collaboration policies on the course website.\n",
    "* All solutions should be written in this notebook.\n",
    "* If you want to modify the skeleton code, you may do so. It has been merely been provided as a framework for your solution.\n",
    "* You may use Python packages for basic linear algebra (e.g. NumPy or SciPy for basic operations), but you may not use packages that directly solve the problem. If you are unsure about using a specific package or function, ask the instructor and/or teaching assistants for clarification.\n",
    "* You must submit this notebook exported as a PDF. You must also submit this notebook as an `.ipynb` file. Submit both files (`.pdf` and `.ipynb`) on Gradescope. **You must mark the PDF pages associated with each question in Gradescope. If you fail to do so, we may dock points.**\n",
    "* It is highly recommended that you begin working on this assignment early.\n",
    "* **Late policy:** a penalty of 10% per day after the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Stereo and Disparity [10 pts]\n",
    "Consider two calibrated cameras whose (virtual) image planes are the z=1 plane, and whose focal points are at (-20, 0, 0) and (20, 0, 0). We''ll call a point in the first camera (x, y), and a point in the second camera (u, v). Points in each camera are relative to the camera center. So, for example if (x, y) = (0, 0), this is really the point (-20, 0, 1) in world coordinates, while if (u, v) = (0, 0) this is the point (20, 0, 1).![Fig1.png](fig/fig1.png)\n",
    "a) Suppose the points (x, y) = (12, 12) is matched to the point (u, v) = (1, 12). What is the 3D location of this point? What is the E matrix linking from Camera 1 to Camera 2? (6pts)\n",
    "\n",
    "b)**EXTRA CREDIT** : Now assume the same setup as before with the same matched points. However Camera 2 is rotated about its center such that it makes an angle of 45 degrees with the +ve x axis. Compute the 3D Location of the point under this assumption, if possible or show why this is not possible. Also compute the E matrix again. (3pts)\n",
    "\n",
    "c) Suppose two calibrated cameras fixate on a point $P$ (see the figure below) in space such that their principal axes (the line passing the optical center and along the viewing direction) intersect at that point. Show that if the image coordinates are normalized so that the coordinate origin $(0,0)$ concides with the principal point (the intersection between the principal axes and the image plane), then the $\\mathbf{E}_{33}$ element of the fundamental matrix is zero.(4pts) ![Fig2.png](fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Sparse Stereo Matching [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will play around with sparse stereo matching methods. You will work on two image pairs, a warrior figure and a figure from the Matrix movies. These files both contain two images, two camera matrices, and associated sets of corresponding points (extracted by manually clicking the images). \n",
    "\n",
    "For the problems below, you will complete functions to demonstrate results on warrior image pairs (warrior1.png, warrior2.png). In all cases, you should apply the same procedures on the matrix image pair (matrix1.png, matrix2.png) as well. (Provide the same thing for BOTH matrix and warrior.) Note that the matrix image pair is harder, in the sense that matching algorithms will not work quite as well on it. You should expect good results, however, on warrior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Detection [3pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we will be using the pre implemented Shi Tomasi corner detector available in OpenCV (https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_shi_tomasi/py_shi_tomasi.html). Make sure to use the exact same version of OpenCV that you used in the previous HW.  \n",
    "In this section you are required to pass the parameters required for corner detection(more information can be found in the link above), and\n",
    "call the OpenCV function which computes the corners and return it as a nice numpy matrix.\n",
    "Example Output:\n",
    "![dinoCorner1](fig/dinoCorner1.png)\n",
    "![dinoCorner2](fig/dinoCorner2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    \"\"\" Convert rgb image to grayscale.\n",
    "    \"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detect(image, nCorners,qualLevel, minDist):\n",
    "     \"\"\"Detect corners on a given image using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        image: Given a grayscale image on which to detect corners.\n",
    "        nCorners: Total number of corners to be extracted.\n",
    "        qualLevel: Quality Level below which corners are detected\n",
    "        minDist: Min Euclidean distance between detected corners.\n",
    "\n",
    "    Returns:\n",
    "        Detected corners (in image coordinate) in a numpy array (nx2).\n",
    "    \"\"\"\n",
    "    '''\n",
    "    Call the OpenCV implementation of the Shi Tomasi Corner Detector\n",
    "    and return the corners. Be wary of the shape returned by the\n",
    "    function. Make sure you are returning a numpy matrix of nx2 dim.\n",
    "    1) The output of the opencv function will be a nump array of nx1x2\n",
    "    To reshape this to a nx2 mtx use .reshape((n,2)) on the matrix\n",
    "    2) The opencv implementation accepts numpy matrices of data type\n",
    "    np.uint8. To convert matrix m to this use, m.astype(np.uint8)\n",
    "    3) Make sure the image is grey scale and is a 2-d image. Use the\n",
    "    function rgb2gray.\n",
    "    '''\n",
    "    assert type(image) == np.ndarray\n",
    "    assert image.ndim == 2\n",
    "    assert 0 < qualLevel < 1\n",
    "    assert minDist > 0\n",
    "        \n",
    "    corners = np.zeros((nCorners,2))\n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corners_result(imgs, corners):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(imgs[0], cmap='gray')\n",
    "    ax1.scatter(corners[0][:, 0], corners[0][:, 1], s=36, edgecolors='r', facecolors='none')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(imgs[1], cmap='gray')\n",
    "    ax2.scatter(corners[1][:, 0], corners[1][:, 1], s=36, edgecolors='r', facecolors='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect corners on warrior and matrix image sets\n",
    "# adjust your corner detection parameters here\n",
    "nCorners = 20\n",
    "qualityLevels = [0.01,0.1,0.2,0.3]\n",
    "minDist = 2\n",
    "#^ Upto you. Decide what works best. Show results for atleast 2\n",
    "#levels.\n",
    "imgs_mat = []\n",
    "imgs_war = []\n",
    "grayimgs_mat = []\n",
    "grayimgs_war = []\n",
    "# Read the two images and convert it to Greyscale\n",
    "for i in range(2):\n",
    "    img_mat = cv2.imread('p4/matrix/matrix' + str(i) + '.png')\n",
    "    gray_mat = cv2.cvtColor(img_mat,cv2.COLOR_BGR2GRAY) \n",
    "    img_mat = cv2.cvtColor(img_mat,cv2.COLOR_BGR2RGB)\n",
    "    imgs_mat.append(img_mat) \n",
    "    grayimgs_mat.append(gray_mat)\n",
    "    # Comment above line and uncomment below line to\n",
    "    # downsize your image in case corner_detect runs slow in test \n",
    "    #grayimgs_mat.append(rgb2gray(img_mat)[::2, ::2])\n",
    "    # if you unleash the power of numpy you wouldn't need to downsize, it'll be fast\n",
    "    img_war = cv2.imread('p4/warrior/warrior' + str(i) + '.png')\n",
    "    gray_war = cv2.cvtColor(img_mat,cv2.COLOR_BGR2GRAY) \n",
    "    img_war = cv2.cvtColor(img_mat,cv2.COLOR_BGR2RGB)\n",
    "    imgs_war.append(img_mat) \n",
    "    grayimgs_war.append(gray_mat)\n",
    "    \n",
    "for qualLevel in qualityLevels:\n",
    "    crns_mat = []\n",
    "    crns_war = []\n",
    "    print (\"Quality Level:\", qualLevel)\n",
    "    for i in range(2):\n",
    "        crns_mat.append(corner_detect(grayimgs_mat[i], nCorners, qualLevel,\\\n",
    "                                      minDist))\n",
    "        crns_war.append(corner_detect(grayimgs_war[i], nCorners, qualLevel,\\\n",
    "                                      minDist))\n",
    "    #show_corners_result(imgs_mat, crns_mat) #uncomment this to show your output!\n",
    "    #show_corners_result(imgs_war, crns_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCC (Normalized Cross-Correlation) Matching [2 pts]\n",
    "\n",
    "Write a function <code>ncc_match</code> that implements the NCC matching algorithm for two input windows.\n",
    "NCC = $\\sum_{i,j}\\tilde{W_1} (i,j)\\cdot \\tilde{W_2} (i,j)$ where $\\tilde{W} = \\frac{W - \\overline{W}}{\\sqrt{\\sum_{k,l}(W(k,l) - \\overline{W})^2}}$ is a mean-shifted and normalized version of the window and $\\overline{W}$ is the mean pixel value in the window W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncc_match(img1, img2, c1, c2, R):\n",
    "    \"\"\"Compute NCC given two windows.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        c1: Center (in image coordinate) of the window in image 1.\n",
    "        c2: Center (in image coordinate) of the window in image 2.\n",
    "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
    "\n",
    "    Returns:\n",
    "        NCC matching score for two input windows.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\"\n",
    "    matching_score = 0\n",
    "    return matching_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test NCC match\n",
    "img1 = np.array([[1, 2, 3, 4], [4, 5, 6, 8], [7, 8, 9, 4]])\n",
    "img2 = np.array([[1, 2, 1, 3], [6, 5, 4, 4], [9, 8, 7, 3]])\n",
    "print (ncc_match(img1, img2, np.array([1, 1]), np.array([1, 1]), 1))\n",
    "# should print 0.8546\n",
    "print (ncc_match(img1, img2, np.array([1, 2]), np.array([1, 2]), 1))\n",
    "# should print 0.8457\n",
    "print (ncc_match(img1, img2, np.array([1, 1]), np.array([1, 2]), 1))\n",
    "# should print 0.6258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Matching [5 pts]\n",
    "\n",
    "Equipped with the corner detector and the NCC matching function, we are ready to start finding correspondances. One naive strategy is to try and find the best match between the two sets of corner points. Write a script that does this, namely, for each corner in image1, find the best match from the detected corners in image2 (or, if the NCC match score is too low, then return no match for that point). You will have to figure out a good threshold (NCCth) value by experimentation. Write a function <code>naive_matching</code> and call it as below. Examine your results for 10, 20, and 30 detected corners in each image. Choose number of detected corners to maximize the number of correct matching pairs. <code>naive_matching</code> will call your NCC matching code. \n",
    "**Properly label or mention which output corresponds to which choice of number of corners. Total number of output is 6 images** (3 choice of number of corners for each matrix and warrior), where one image is like below:\n",
    "\n",
    "\n",
    "Number of Corners: 10\n",
    "<img src = \"fig/dinoMatch.png\" alt=\"dino match\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are free to modify code here, create your helper functions etc.\n",
    "# detect corners on warrior and matrix sets\n",
    "nCorners = 20 # Do this for 10, 20 and 30 corners\n",
    "qualityLevels = 0.01 #Adjust to what you feel is the best\n",
    "minDist = 2 #Adjust to what you feel is the best\n",
    "\n",
    "crns_mat = []\n",
    "crns_war = []\n",
    "#Detect corners on the already read images\n",
    "for i in range(2):\n",
    "    crns_mat.append(corner_detect(grayimgs_mat[i], nCorners, qualLevel,\\\n",
    "                                      minDist))\n",
    "    crns_war.append(corner_detect(grayimgs_war[i], nCorners, qualLevel,\\\n",
    "                                      minDist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match corners\n",
    "R = 15\n",
    "NCCth = 0.7 # Put your threshold\n",
    "matching_mat = naive_matching(imgs_mat[0]/255, imgs_mat[1]/255, crns_mat[0], crns_mat[1], R, NCCth)\n",
    "matching_war = naive_matching(imgs_war[0]/255, imgs_war[1]/255, crns_war[0], crns_war[1], R, NCCth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matching result\n",
    "def show_matching_result(img1, img2, matching):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.hstack((img1, img2)), cmap='gray') # two dino images are of different sizes, resize one before use\n",
    "    for p1, p2 in matching:\n",
    "        plt.scatter(p1[0], p1[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.scatter(p2[0] + img1.shape[1], p2[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.plot([p1[0], p2[0] + img1.shape[1]], [p1[1], p2[1]])\n",
    "    plt.savefig('dino_matching.png')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to show output\n",
    "#print(\"Number of Corners:\", nCorners)\n",
    "#show_matching_result(imgs_mat[0], imgs_mat[1], matching_mat)\n",
    "#show_matching_result(imgs_war[0], imgs_war[1], matching_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epipolar Geometry [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue our quest on furthering matching performance, we turn to epipolar geometry! The first step is to compute the Fundamental Matrix. Note that the Fundamental matrix is very similar to the Essential Matrix which you should be familiar about in class, however the Fundamental Matrix. does not require the camera to be calibrated (which is generally the case!)  \n",
    "The code to compute the fundamental matrix is provided for you. Go through it and make sure the steps make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def compute_fundamental(x1,x2):\n",
    "    \"\"\"    Computes the fundamental matrix from corresponding points \n",
    "        (x1,x2 3*n arrays) using the 8 point algorithm.\n",
    "        Each row in the A matrix below is constructed as\n",
    "        [x'*x, x'*y, x', y'*x, y'*y, y', x, y, 1] \n",
    "\n",
    "        Returns:\n",
    "        Fundamental Matrix (3x3)\n",
    "\n",
    "    \"\"\"  \n",
    "    A = np.array([x1[0,:]*x2[0,:],x1[0,:]*x2[1,:],x1[0,:]*x2[2,:],\\\n",
    "                  x1[1,:]*x2[0,:],x1[1,:]*x2[1,:],x1[1,:]*x2[2,:],\\\n",
    "                  x1[2,:]*x2[0,:],x1[2,:]*x2[1,:],x1[2,:]*x2[2,:]]).T\n",
    "    _,_,vh = np.linalg.svd(A)\n",
    "    F = vh[-1].reshape(3,3)\n",
    "    u,sig,vh = np.linalg.svd(F)\n",
    "    sig[2] = 0\n",
    "    F = u@np.diag(sig)@vh\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "    return F\n",
    "\n",
    "def fundamental_matrix(x1,x2):\n",
    "    # Normalization of the corner points is handled here\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "\n",
    "    # normalize image coordinates\n",
    "    x1 = x1 / x1[2]\n",
    "    mean_1 = np.mean(x1[:2],axis=1)\n",
    "    S1 = np.sqrt(2) / np.std(x1[:2])\n",
    "    T1 = np.array([[S1,0,-S1*mean_1[0]],[0,S1,-S1*mean_1[1]],[0,0,1]])\n",
    "    x1 = np.dot(T1,x1)\n",
    "    \n",
    "    x2 = x2 / x2[2]\n",
    "    mean_2 = np.mean(x2[:2],axis=1)\n",
    "    S2 = np.sqrt(2) / np.std(x2[:2])\n",
    "    T2 = np.array([[S2,0,-S2*mean_2[0]],[0,S2,-S2*mean_2[1]],[0,0,1]])\n",
    "    x2 = np.dot(T2,x2)\n",
    "\n",
    "    # compute F with the normalized coordinates\n",
    "    F = compute_fundamental(x1,x2)\n",
    "\n",
    "    # reverse normalization\n",
    "    F = np.dot(T1.T,np.dot(F,T2))\n",
    "\n",
    "    return F/F[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Epipolar Lines [5 pts]\n",
    "Using this fundamental matrix, plot the epipolar lines in both image pairs across all images. For this part you may want to complete the function <code>plot_epipolar_lines</code>. Shown your result for matrix and warrior as the figure below. \n",
    "![Dino Epipolar](fig/dinoEpi1.png)\n",
    "![Dino Epipolar](fig/dinoEpi2.png)\n",
    "\n",
    "Also, write the script to calculate the epipoles for a given Fundamental matrix and corner point correspondences in the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epipole(F):\n",
    "    '''\n",
    "    This function computes the epipoles for a given fundamental matrix \n",
    "    and corner point correspondences\n",
    "    input:\n",
    "    F--> Fundamental matrix\n",
    "    output:\n",
    "    e1--> corresponding epipole in image 1\n",
    "    e2--> epipole in image2\n",
    "    '''\n",
    "    #your code here\n",
    "    return e1,e2\n",
    "\n",
    "def plot_epipolar_lines(img1,img2, cor1, cor2):\n",
    "    \"\"\"Plot epipolar lines on image given image, corners\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        cor1: Corners in homogeneous image coordinate in image 1 (3xn)\n",
    "        cor2: Corners in homogeneous image coordinate in image 2 (3xn)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the epipolar lines for the matched corners. No need to\n",
    "#alter the below code. \n",
    "imgids = [\"dino\", \"matrix\", \"warrior\"]\n",
    "for imgid in imgids:\n",
    "    \n",
    "    I1 = cv2.imread(\"./p4/\"+imgid+\"/\"+imgid+\"0.png\") \n",
    "    I1 = cv2.cvtColor(I1,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    I2 = cv2.imread(\"./p4/\"+imgid+\"/\"+imgid+\"1.png\") \n",
    "    I2 = cv2.cvtColor(I1,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    cor1 = np.load(\"./p4/\"+imgid+\"/cor1.npy\")\n",
    "    cor2 = np.load(\"./p4/\"+imgid+\"/cor2.npy\")\n",
    "    plot_epipolar_lines(I1,I2,cor1,cor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Rectification [5 pts]\n",
    " An interesting case for epipolar geometry occurs when two images are parallel to each other. In this case, there is no rotation component involved between the two images and the essential matrix is $\\texttt{E}=[\\boldsymbol{T_{x}}]\\boldsymbol{R}=[\\boldsymbol{T_{x}}]$. Also if you observe the epipolar lines $\\boldsymbol{l}$ and $\\boldsymbol{l^{'}}$ for parallel images, they are horizontal and consequently, the corresponding epipolar lines share the same vertical coordinate. Therefore the process of making images parallel becomes useful while discerning the relationships between corresponding points in images.\n",
    " Rectifying a pair of images can also be done for uncalibrated camera images (i.e. we do not require the camera matrix of intrinsic parameters). Using the fundamental matrix we can find the pair of epipolar lines $\\boldsymbol{l_i}$ and $\\boldsymbol{l^{'}_i}$ for each of the correspondences. The intersection of these lines will give us the respective epipoles $\\boldsymbol{e}$ and $\\boldsymbol{e^{'}}$.  Now to make the epipolar lines to be parallel we need to map the epipoles to infinity. Hence , we need to find a homography that maps the epipoles to infinity. The method to find the homography has been implemented for you. You can read more about the method used to estimate the homography in the paper \"Theory and Practice of Projective Rectification\" by Richard Hartley.\n",
    " ![Image Rectification Setup](image_rectification.png)\n",
    " Using the compute_epipoles function from the previous part and the given compute_matching_homographies function, find the rectified images and plot the parallel epipolar lines using the plot_epipolar_lines function from above. You need to do this for both the matrix and the warrior images. A sample output will look as below:\n",
    " ![sample rectification](Sample_rectification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matching_homographies(e2, F, im2, points1, points2):\n",
    "    \n",
    "    '''This function computes the homographies to get the rectified images\n",
    "    input:\n",
    "    e2--> epipole in image 2\n",
    "    F--> the Fundamental matrix (Think about what you should be passing F or F.T!)\n",
    "    im2--> image2\n",
    "    points1 --> corner points in image1\n",
    "    points2--> corresponding corner points in image2\n",
    "    output:\n",
    "    H1--> Homography for image 1\n",
    "    H2--> Homography for image 2\n",
    "    '''\n",
    "    # calculate H2\n",
    "    width = im2.shape[1]\n",
    "    height = im2.shape[0]\n",
    "\n",
    "    T = np.identity(3)\n",
    "    T[0][2] = -1.0 * width / 2\n",
    "    T[1][2] = -1.0 * height / 2\n",
    "\n",
    "    e = T.dot(e2)\n",
    "    e1_prime = e[0]\n",
    "    e2_prime = e[1]\n",
    "    if e1_prime >= 0:\n",
    "        alpha = 1.0\n",
    "    else:\n",
    "        alpha = -1.0\n",
    "\n",
    "    R = np.identity(3)\n",
    "    R[0][0] = alpha * e1_prime / np.sqrt(e1_prime**2 + e2_prime**2)\n",
    "    R[0][1] = alpha * e2_prime / np.sqrt(e1_prime**2 + e2_prime**2)\n",
    "    R[1][0] = - alpha * e2_prime / np.sqrt(e1_prime**2 + e2_prime**2)\n",
    "    R[1][1] = alpha * e1_prime / np.sqrt(e1_prime**2 + e2_prime**2)\n",
    "\n",
    "    f = R.dot(e)[0]\n",
    "    G = np.identity(3)\n",
    "    G[2][0] = - 1.0 / f\n",
    "\n",
    "    H2 = np.linalg.inv(T).dot(G.dot(R.dot(T)))\n",
    "\n",
    "    # calculate H1\n",
    "    e_prime = np.zeros((3, 3))\n",
    "    e_prime[0][1] = -e2[2]\n",
    "    e_prime[0][2] = e2[1]\n",
    "    e_prime[1][0] = e2[2]\n",
    "    e_prime[1][2] = -e2[0]\n",
    "    e_prime[2][0] = -e2[1]\n",
    "    e_prime[2][1] = e2[0]\n",
    "\n",
    "    v = np.array([1, 1, 1])\n",
    "    M = e_prime.dot(F) + np.outer(e2, v)\n",
    "\n",
    "    points1_hat = H2.dot(M.dot(points1.T)).T\n",
    "    points2_hat = H2.dot(points2.T).T\n",
    "\n",
    "    W = points1_hat / points1_hat[:, 2].reshape(-1, 1)\n",
    "    b = (points2_hat / points2_hat[:, 2].reshape(-1, 1))[:, 0]\n",
    "\n",
    "    # least square problem\n",
    "    a1, a2, a3 = np.linalg.lstsq(W, b)[0]\n",
    "    HA = np.identity(3)\n",
    "    HA[0] = np.array([a1, a2, a3])\n",
    "\n",
    "    H1 = HA.dot(H2).dot(M)\n",
    "    return H1, H2\n",
    "\n",
    "def image_rectification(im1,im2,points1,points2):\n",
    "    '''this function provides the rectified images along with the new corner points as outputs for a given pair of \n",
    "    images with corner correspondences\n",
    "    input:\n",
    "    im1--> image1\n",
    "    im2--> image2\n",
    "    points1--> corner points in image1\n",
    "    points2--> corner points in image2\n",
    "    outpu:\n",
    "    rectified_im1-->rectified image 1\n",
    "    rectified_im2-->rectified image 2\n",
    "    new_cor1--> new corners in the rectified image 1\n",
    "    new_cor2--> new corners in the rectified image 2\n",
    "    '''\n",
    "    \"your code here\"\n",
    "    '''\n",
    "    The steps you need to do are outlined as follows\n",
    "    1) Get the F matrix by calling fundamental_matrix\n",
    "    2) Compute the epipoles(pass F.T to the function)\n",
    "    3) Call the compute matching homographies function passing\n",
    "    in F.T (Think about this!), and the other required parameters.\n",
    "    4)Based on that homography warp the source image to the target\n",
    "    image\n",
    "    5)Return the new corners in this image(DONT CALL CORNER DETECT AGAIN)\n",
    "    along with the rectified images\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return rectified_im1,rectified_im2,new_cor1,new_cor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Using epipolar geometry[5 pts]\n",
    "\n",
    "We will now use the epipolar geometry constraint on the rectified images and updated corner points to build a better matching algorithm. First, detect 10 corners\n",
    "in Image1. Then, for each corner, do a linesearch along the corresponding parallel epipolar line in Image2.\n",
    "Evaluate the NCC score for each point along this line and return the best match (or no match if all\n",
    "scores are below the NCCth). R is the radius (size) of the NCC patch in the code below.  You do not\n",
    "have to run this in both directions. Show your result as in the naive matching part. Execute this for the warrior and matrix images (**Total two outputs images**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_correspondence(img1, img2, corrs):\n",
    "    \"\"\"Plot matching result on image pair given images and correspondences\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corrs: Corner correspondence\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here.\n",
    "    You may refer to the show_matching_result function\n",
    "    \"\"\"\n",
    "\n",
    "def correspondence_matching_epipole(img1, img2, corners1, F, R, NCCth):\n",
    "    \"\"\"Find corner correspondence along epipolar line.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corners1: Detected corners in image 1.\n",
    "        F: Fundamental matrix calculated using given ground truth corner correspondences.\n",
    "        R: NCC matching window radius.\n",
    "        NCCth: NCC matching threshold.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        Matching result to be used in display_correspondence function\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Your code here.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1 = cv2.imread(\"./p4/matrix/matrix0.png\") \n",
    "I1 = cv2.cvtColor(I1,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "I2 = cv2.imread(\"./p4/matrix/matrix1.png\") \n",
    "I2 = cv2.cvtColor(I1,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "cor1 = np.load(\"./p4/matrix/cor1.npy\")\n",
    "cor2 = np.load(\"./p4/matrix/cor2.npy\")\n",
    "\n",
    "\n",
    "I3 = cv2.imread(\"./p4/warrior/warrior0.png\") \n",
    "I3 = cv2.cvtColor(I3,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "I4 = cv2.imread(\"./p4/warrior/warrior1.png\") \n",
    "I4 = cv2.cvtColor(I4,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "cor3 = np.load(\"./p4/warrior/cor1.npy\")\n",
    "cor4 = np.load(\"./p4/warrior/cor2.npy\")\n",
    "\n",
    "# For matrix\n",
    "rectified_im1,rectified_im2,new_cor1,new_cor2 = image_rectification(I1,I2,cor1,cor2)\n",
    "F_new = fundamental_matrix(new_cor1, new_cor2)\n",
    "\n",
    "nCorners = 10\n",
    "# Choose your threshold\n",
    "NCCth = 0.7\n",
    "#decide the NCC matching window radius\n",
    "R = 10\n",
    "# detect corners using corner detector here, store in corners1\n",
    "corners1 = corner_detect(rectified_im1, nCorners, qualLevel, minDist)\n",
    "corrs = correspondence_matching_epipole(rectified_im1, rectified_im2, corners1, F_new, R, NCCth)\n",
    "display_correspondence(rectified_im1, rectified_im2, corrs)\n",
    "\n",
    "\n",
    "# For warrior\n",
    "rectified_im3,rectified_im4,new_cor3,new_cor4 = image_rectification(I3,I4,cor3,cor4)\n",
    "F_new2=fundamental_matrix(new_cor3, new_cor4)\n",
    "# You may wish to change your NCCth and R for warrior here.\n",
    "corners2 = corner_detect(rectified_im3, nCorners, qualLevel, minDist)\n",
    "corrs = correspondence_matching_epipole(rectified_im3, rectified_im4, corners2, F_new2, R, NCCth)\n",
    "display_correspondence(rectified_im3, rectified_im4, corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: RANSAC for Estimating the Fundamental Matrix [15 pts]\n",
    "We will now use SIFT to detect and match features, then use RANSAC to eliminate outliers that do not conform to a fundamental matrix model. For this problem, we are providing matched SIFT points in text files that you may simply read as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of matching points [5 pts]\n",
    "Use the provided matched SIFT points in the two images road1.png (leftimage) and road2.png (right image). Visualize the matched features by drawing lines between the left and right images. You may use the provided *show_matching_result* function. The data in points1.txt are the keypoints in the left image and the data in points2.txt are the keypoints in the right image. Each row has the x and y coordinates for a point. Corresponding rows in the two files are the matching points. Randomly visualize 20 matchings from all matched points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "x1 = np.loadtxt(\"points1.txt\").T\n",
    "x2 = np.loadtxt(\"points2.txt\").T\n",
    "roadimgs = []\n",
    "for i in range(2):\n",
    "    img = cv2.imread('road' + str(i+1) + '.png',0)\n",
    "    roadimgs.append(img)\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC with 8-point algorithm [10 pts]\n",
    "Use RANSAC with the 8-point algorithm to remove outliers and re-estimate the fundamental matrix with the inliers. Visualize the inlier matches by drawing lines between the left and right images. Plot the epipolar lines for 5 randomly selected keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_fundamental_RANSAC(cor1, cor2, epiConstThres, nSample):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - cor1, cor2: corners in image1 and image2\n",
    "    - epiConstThres: Threshold for accepting inliers\n",
    "    - nSample: number of iterations for RANSAC\n",
    "    \n",
    "    Output:\n",
    "    - bestF: best fundamental matrix\n",
    "    - bestInliersIdx: under bestF, the index of inliers of matching points\n",
    "    - bestInliersNumList: record the best number of inliers so far at each iteration, with length nSample\n",
    "    \"\"\"\n",
    "    \"\"\" YOUR CODE HERE !!!\"\"\"\n",
    "    '''\n",
    "    The steps for the procedure is as follows.\n",
    "    1)Randomly select 8 matching pairs from the 2 sets of corners\n",
    "    2)Call the function compute_fundamental to est the F matrix\n",
    "    from these random 8 matching pairs.\n",
    "    3)Evaluate the epipolar constraints for the points by evaluating\n",
    "    the expression p'^TFp\n",
    "    4)If this is lesser than epiConstThres add to inliers list\n",
    "    5)Repeat the above steps nSamples number of times\n",
    "    6)Return the bestF (one with highest inliers), the corr indices\n",
    "    of the best inliers, and the number of inliers as a function of\n",
    "    nSamples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    bestInliersNumList = []\n",
    "\n",
    "    return bestF, bestInliersIdx, bestInliersNumList\n",
    "\n",
    "def fundamental_matrix_RANSAC(x1,x2, epiConstThres, nSample):\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "\n",
    "    # normalize image coordinates\n",
    "    x1 = x1 / x1[2]\n",
    "    mean_1 = np.mean(x1[:2],axis=1)\n",
    "    S1 = np.sqrt(2) / np.std(x1[:2])\n",
    "    T1 = np.array([[S1,0,-S1*mean_1[0]],[0,S1,-S1*mean_1[1]],[0,0,1]])\n",
    "    x1 = np.dot(T1,x1)\n",
    "    \n",
    "    x2 = x2 / x2[2]\n",
    "    mean_2 = np.mean(x2[:2],axis=1)\n",
    "    S2 = np.sqrt(2) / np.std(x2[:2])\n",
    "    T2 = np.array([[S2,0,-S2*mean_2[0]],[0,S2,-S2*mean_2[1]],[0,0,1]])\n",
    "    x2 = np.dot(T2,x2)\n",
    "\n",
    "    # compute F with the normalized coordinates\n",
    "    bestF, bestInliersIdx, bestInliersNumList = compute_fundamental_RANSAC(x1,x2,epiConstThres,nSample)\n",
    "    \n",
    "    # reverse normalization\n",
    "    bestF = np.dot(T1.T,np.dot(bestF,T2))\n",
    "\n",
    "    return bestF/bestF[2,2], bestInliersIdx, bestInliersNumList\n",
    "\n",
    "# calculating F using RANSAC\n",
    "epiConstThres = 0.1\n",
    "nSample = 3000\n",
    "np.random.seed(10)\n",
    "F, bestInliersIdx, bestInliersNumList = fundamental_matrix_RANSAC(x1_h, x2_h, epiConstThres, nSample)\n",
    "inlierPts1 = x1_h[:,bestInliersIdx]\n",
    "inlierPts2 = x2_h[:,bestInliersIdx]\n",
    "chooseidx = np.random.choice(inlierPts1.shape[1], 5, replace=False)\n",
    "plot_epipolar_lines(F, roadimgs[0], roadimgs[1], inlierPts1[:,chooseidx], inlierPts2[:,chooseidx])\n",
    "\n",
    "print('Number of inliers as iteration increases:')\n",
    "plt.plot(np.arange(len(bestInliersNumList)), bestInliersNumList, 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Instructions\n",
    "Remember to submit a PDF version of this notebook to Gradescope. Please make sure the contents in each cell are clearly shown in your final PDF file.\n",
    "\n",
    "There are multiple options for converting the notebook to PDF:\n",
    "1. You can find the export option at File $\\rightarrow$ Download as $\\rightarrow$ PDF via LaTeX\n",
    "2. You can first export as HTML and then convert to PDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
